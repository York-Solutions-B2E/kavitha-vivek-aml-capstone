{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7af65f2-19c1-41b0-a779-6d8fa4baabdf",
   "metadata": {},
   "source": [
    "Customer Propensity â€“ Google Analytics Sample Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f3365e-aa01-4a30-8745-17cd7d7f69a8",
   "metadata": {},
   "source": [
    "Goal : The goal of this project is to create a machine language model to find whether a customer will perform a 'hit to cart' action to send ads to increase the return on ad-spending of the company.\n",
    "\n",
    "Given: Google analytics dataset of a eCommerce store for a period of August 2016 to July 2017\n",
    "\n",
    "Project Plan : \n",
    "\n",
    "1. Analyse the given data \n",
    "    - Find the number of fields \n",
    "    - See the dimensionality of the dataset\n",
    "    - Look for null values \n",
    "    - Check for duplicate values\n",
    "    - Find the unique values of each field\n",
    "    \n",
    "2. Create a vertex AI instance\n",
    "    In the google cloud console, create a vertex AI instance with the following configuration\n",
    "    Instance Name : kavitha-instance\n",
    "    Region : us-east4\n",
    "    Version : M115\n",
    "    Machine Type : e2-standard-4 (4VCPUs, 16GB RAM)\n",
    "    Storage : 100GB\n",
    "    Other settings : Set to default\n",
    "    \n",
    "3. Write sql queries in the bigquery console to understand the data schema\n",
    "\n",
    "4. Perform EDA in vertex jupyter notebook to understand the relationship and data distribution of the given dataset\n",
    "\n",
    "5. Select features those are likely to be important\n",
    "\n",
    "6. To create a BigQueryML model :\n",
    "    - Install required dependencies and packages\n",
    "    - Import necessary python modules\n",
    "    - Create a cloud storage bucket\n",
    "        Name : york-bb-cohort-kavi_capstone_bucket\n",
    "        Region : us-east4\n",
    "    - Create a bigquery dataset\n",
    "        Name : york-bb-cohort:kavitha_capstone_bucket\n",
    "        Location : US\n",
    "    - Create a view with all the selected features\n",
    "    - Create a model\n",
    "        XGBoostClassifier\n",
    "        LogisticRegressionClassifier\n",
    "    - Train and evaluate the model\n",
    "    - Calculate performance metrics\n",
    "    - Make model predictions on test data\n",
    "    - Try with the different set of features for both the models\n",
    "    - Hyperparameter tuning  to improve the model performance\n",
    "    - Choose the best model and deploy it for prediction\n",
    "    \n",
    "7. Deployment :\n",
    "    - Export the model to the cloud storage bucket\n",
    "    - Upload the model to vertex AI\n",
    "    - create an endpoint and deploy the model\n",
    "    \n",
    "8. Use the model for online predictions :\n",
    "    - Identify the categorical features used in the model and encode it using the model dictionary created during deployment\n",
    "    - Create a query and run online prediction\n",
    "    \n",
    "Conclusion : Based on the model prediction, if a visitor is predicted to add to cart during a given visit they can be targeted for marketing campaign. Given only a small percentage of visitors actually add an item to cart, this will make the ad-spending more effective and increase the revenue.\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-root-py",
   "name": "workbench-notebooks.m115",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/workbench-notebooks:m115"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
